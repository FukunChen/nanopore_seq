import pod5
import pysam
import numpy as np
import json
from tqdm import tqdm
from remora import io

# è¾“å…¥è·¯å¾„
pod5_path = "/mnt/d/nanopore/data/pod5/dIdU_MIX/merged_didumix.pod5"
bam_path = "/mnt/d/nanopore/calls_didumix.bam/didumix_onlydamage.bam"

# æ‰“å¼€æ–‡ä»¶
pod5_dr = pod5.DatasetReader(pod5_path)
bam_fh = io.ReadIndexedBam(bam_path)

# è·å–æ‰€æœ‰ BAM read_id
bam_read_ids = list(bam_fh.read_ids)
total_reads = len(bam_read_ids)
print(f"ğŸ“Œ BAM contains {total_reads} reads")

# æ¯ 5% ä¸€æ‰¹ï¼ˆå…± 20 æ‰¹ï¼‰
batch_size = total_reads // 20

# éå†æ¯ä¸ª batch
for batch_idx in range(20):
    start = batch_idx * batch_size
    end = (batch_idx + 1) * batch_size if batch_idx < 19 else total_reads
    batch_read_ids = bam_read_ids[start:end]

    output_jsonl = f"ref_views_didumix_batch_{batch_idx+1}.jsonl"
    print(f"\nğŸš€ Processing batch {batch_idx+1}/20: {len(batch_read_ids)} reads â†’ {output_jsonl}")

    with open(output_jsonl, "w") as out_f:
        for read_id in tqdm(batch_read_ids,
                            total=len(batch_read_ids),
                            desc=f"[Batch {batch_idx+1:02d}/20]",
                            ncols=100):
            # è·å– BAM read
            bam_read = bam_fh.get_first_alignment(read_id)
            if bam_read is None:
                continue

            # è·å– pod5 read
            try:
                pod5_read = pod5_dr.get_read(read_id)
            except KeyError:
                continue

            try:
                # æ„é€  io_read
                io_read = io.Read.from_pod5_and_alignment(pod5_read, bam_read)
                if io_read.ref_reg is None:
                    continue

                # è°ƒæ•´ reference window åŒºé—´
                region = io_read.ref_reg.adjust(start_adjust=0, end_adjust=200 - io_read.ref_reg.len)
                ref_view = io_read.extract_ref_reg(region)

                # æ„é€ ç»“æœå­—å…¸
                ref_view_data = {
                    "raw_signal": io_read.dacs.tolist(),
                    "query_to_signal": io_read.query_to_signal.tolist(),
                    "read_id": ref_view.read_id,
                    "sequence": ref_view.seq,
                    "signal": ref_view.norm_signal.tolist(),
                    "seq_to_sig_map": ref_view.seq_to_sig_map.tolist(),
                    "ref_region": {
                        "contig": ref_view.ref_reg.ctg,
                        "strand": ref_view.ref_reg.strand,
                        "start": ref_view.ref_reg.start,
                        "end": ref_view.ref_reg.end,
                    },
                    "sig_start": int(ref_view.sig_start),
                }

                # å†™å…¥ä¸€è¡Œ JSON
                out_f.write(json.dumps(ref_view_data) + "\n")

            except Exception:
                continue  # å‡ºé”™çš„ read è·³è¿‡

    print(f"âœ… Finished batch {batch_idx+1}, saved to {output_jsonl}")